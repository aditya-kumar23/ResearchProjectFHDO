\chapter{Evaluation}
\label{ch:evaluation}

This chapter evaluates the C-SLAM backends by instantiating the defined experimental design space \(\mathcal{D}=\mathcal{A}\times\mathcal{N}\times\mathcal{Q}\times\mathcal{I}\) in Section~\ref{sec:research_design} with a baseline grid (clean network, reference QoS) and targeted sweeps (team size, QoS, impairments).

A comparison between centralised and decentralised backends often comes with default expectations; for example, centralised is accurate but bandwidth-heavy, whereas decentralised is scalable but slower to settle. Rather than foregrounding these obvious points, the discussion emphasises the non-obvious implications that emerge when accuracy, delivery, and semantics-aware timing are interpreted jointly: (i) \emph{what ``latency'' means} depends on the architecture's communication semantics, (ii) \emph{dataset regime and constraint structure} can dominate ``architecture'' effects, and (iii) \emph{robustness} is shaped by both estimator behaviour and software stability under backlog and intermittency.

\section{Experimental Protocol}
\label{sec:eval_protocol}

All experiments replay the same dataset-provided constraint stream (Section~\ref{sec:dataset_frontend}) and log metrics with the KPI pipeline described in Section~\ref{sec:metrics_implementation}. The dataset contains two native regimes (\emph{WiFi} and \emph{ProRadio}). These are treated as environmental conditions, not tunable parameters. Each configuration is repeated five times (\(R=5\)), and tables report repeat means with 95\% confidence intervals where shown.

The baseline uses the reference ROS\,2 QoS profile RV-20 (reliable, volatile, depth 20). The QoS sweep varies only \(\mathcal{Q}\) at a fixed team size and regime. The impairment sweep varies only \(\mathcal{I}\) using the mechanism defined in Section~\ref{sec:network_impairment_qos}; in the runs reported here, impairments are applied at \emph{factor publication} (publisher-impaired mode), i.e., they primarily constrain \emph{constraint availability} rather than directly impairing decentralised peer exchange or centralised downlink.

\subsection{Timing Semantics}
\label{sec:eval_semantics}

Timing metrics are reported in a way that respects architectural semantics (Section~\ref{sec:timing_metrics}). The centralised backend exposes a clear dissemination event: the server broadcasts a global update. We therefore report \(t_{\mathrm{global}}\) as a \emph{dissemination-tail proxy} (time until the last broadcast observed in the run). In addition, ``Corr.\ p95'' reports loop-closure correction latency (p95), i.e., the time from loop-closure ingestion until that correction is stably reflected.

The decentralised backend has no single broadcast event. For decentralised runs, ``Corr.\ p95'' refers to interface correction stabilisation latency (p95) and should be read as a \emph{correction-propagation proxy}. These two ``Corr.\ p95'' numbers quantify different pipeline stages and must not be compared as if they were the same definition of convergence.

When a backend-agnostic convergence statement is required, the chapter uses the stabilisation proxy \(T_{\mathrm{conv}}^{(\mathrm{team})}=\max_r T_{\mathrm{conv}}^{(r)}\) from Section~\ref{sec:timing_metrics}. Importantly, event-style proxies (such as \(t_{\mathrm{global}}\) or ``Iface p95'') can move in counter-intuitive directions under missing constraints: for instance, a blackout may shorten \(t_{\mathrm{global}}\) simply because fewer factors arrive (less work and fewer broadcasts), even though estimation quality degrades. Throughout this chapter, timing is therefore interpreted jointly with delivery and accuracy.

\section{Results Overview}
\label{sec:eval_overview}

Across all studies, three patterns recur and shape the interpretation.

First, the dominant ``latency'' in these backends is not the solver update duration (which is short in both cases), but the \emph{time to a team-visible stable correction}. Centralised runs typically settle rapidly once a broadcast arrives (very small \(T_{\mathrm{conv}}^{(\mathrm{team})}\)), so perceived delay is largely driven by \emph{when} broadcasts occur. Decentralised runs often show shorter optimisation steps but longer tails for interface-level stabilisation, so perceived delay is dominated by \emph{propagation and settling} rather than raw compute.

Second, the dataset regime interacts strongly with architecture: the WiFi and ProRadio sequences differ in their constraint-stream characteristics, and this can dominate scalability trends. In particular, the decentralised backend behaves qualitatively differently at r5 under WiFi vs.\ ProRadio, despite identical algorithmic structure and logging.

Third, robustness is not only about estimation under missing factors; it is also about whether the implementation remains stable under backlog, delayed delivery, or intermittency. The decentralised backend exhibits a repeatable crash signature in several blackout and QoS configurations; these cases are treated as unsupported in the quantitative comparisons and are explicitly analysed as an engineering limitation.

\section{Baseline Comparison Under Native Dataset Networking}
\label{sec:eval_baseline}

The baseline establishes a common operating point for both architectures under the dataset-native regimes, using identical factor streams and the same RV-20 QoS profile. It anchors the subsequent team-size, QoS, and impairment sweeps.

\subsection{Reference r3 Baseline: Centralised vs.\ Decentralised}
\label{subsec:eval_r3_baseline}

Table~\ref{tab:eval_baseline_r3} summarises the r3 baseline. Under both regimes, the centralised backend achieves lower ATE RMSE than the decentralised backend. The more informative observation, however, is \emph{how} this gap coexists with timing and traffic:

\emph{(i) ``Faster corrections'' is not the same as ``faster convergence''.}
On WiFi r3, the decentralised ``Corr.\ p95'' is smaller than the ProRadio case (4.79\,s vs.\ 14.37\,s), yet the shared convergence proxy \(T_{\mathrm{conv}}^{(\mathrm{team})}\) remains an order of magnitude larger than the centralised value. This reflects the decentralised system's correction propagation and settling tail: local updates are frequent and cheap, but the team-level estimate takes longer to stop moving under the stabilisation criterion.

\emph{(ii) ``Decentralised uses less bandwidth'' is regime-dependent.}
On WiFi r3, decentralised traffic is lower than centralised traffic (28.7 vs.\ 55.0\,MiB). On ProRadio r3, decentralised traffic is slightly higher than centralised traffic (32.2 vs.\ 30.3\,MiB). In other words, the decentralised pipeline does not guarantee a bandwidth win; it shifts \emph{where} bytes are spent (peer-to-peer summaries and factor exchange vs.\ downlink broadcast), and the regime can flip which side is cheaper.

\emph{(iii) Distributed compute does not imply ``less compute''.}
Figure~\ref{fig:eval_efficiency_cpu_s_per_step} reports a normalised view (CPU-seconds per solver update) to separate ``how expensive is an update'' from ``how many updates happen''.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{src/img/main/eval_efficiency_cpu_s_per_step.pdf}
	\caption{Efficiency-normalised metric for the r3 baseline: CPU-seconds per optimisation step. Bars show mean over repeats ($R=5$); error bars denote 95\% CI across repeats, with individual repeats overlaid. Lower values indicate less compute time spent per solver update.}
	\label{fig:eval_efficiency_cpu_s_per_step}
\end{figure}

\begin{table}[H]
	\centering
	\caption{Baseline comparison for r3 under dataset-native networking. \(t_{\mathrm{global}}\) is a centralised dissemination proxy (time until last global map broadcast). ``Corr.\ p95'' is a correction-latency indicator. For centralised runs it reports loop-closure correction latency (p95), while for decentralised runs it reports interface correction stabilisation latency (p95). These timing indicators are architecture-specific. Although Strict cross-architecture convergence comparisons should use the shared stabilisation proxy \(T_{\mathrm{conv}}^{(\mathrm{team})}\) (Section~\ref{sec:eval_semantics}). Link(L): ProRadio(P) \& WiFi (W); Backend(B): Centralised(C) \& Decentralised(D).}
	\label{tab:eval_baseline_r3}
	\small
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{llrrrrr}
		\toprule
		L & B & ATE RMSE (m) & \(t_{\mathrm{global}}\) (s) & Corr.\ p95 (s) & \(T_{\mathrm{conv}}^{(\mathrm{team})}\) (s) & Traffic (MiB) \\
		\midrule
		P & C & 2.164 $\pm$ 0.000 & 11.5 & 0.096 & 0.01 $\pm$ 0.00 & 30.3 \\
		P & D & 3.821 $\pm$ 0.072 & --   & 14.37 & $\geq$ 21.33 $\pm$ 5.94 & 32.2 \\
		W & C & 2.270 $\pm$ 0.002 & 16.4 & 0.126 & 0.12 $\pm$ 0.00 & 55.0 \\
		W & D & 3.581 $\pm$ 0.035 & --   & 4.79  & 15.19 $\pm$ 3.53 & 28.7 \\
		\bottomrule
	\end{tabular}
	\vspace{2pt}
	\footnotesize\emph{Traffic split:} centralised (up/down) ProRadio 4.1/26.2, WiFi 5.6/49.4; decentralised up=total, down=0.
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.98\linewidth]{src/img/main/eval_r3_summary.pdf}
	\caption{Compact summary of the r3 baseline (Table~\ref{tab:eval_baseline_r3}): ATE RMSE and total traffic with uplink/downlink split. Bars show mean over repeats ($R=5$); error bars denote 95\% CI across repeats.}
	\label{fig:eval_r3_summary}
\end{figure}

Detailed resource traces used for diagnosing stability and backlog effects are provided in Appendix Figure~\ref{fig:eval_resources_timeseries}.

\subsection{Scalability Trend (r3--r5)}
\label{subsec:eval_scalability}

Scalability is assessed by repeating the baseline protocol at r4 and r5. Table~\ref{tab:eval_baseline_scalability_condensed} reports the key trend metrics used in the main discussion; the full multi-metric tables (including CPU/RSS and Opt.\ p95) are provided in Appendix Tables~\ref{tab:eval_baseline_scalability_full_a}--\ref{tab:eval_baseline_scalability_full_b}. For the decentralised backend, Figure~\ref{fig:eval_ddf_ate_vs_stop_time} visualises how final accuracy relates to DDF termination time as team size increases. Diagnostic counts of inter-robot loop closures and interface-update activity are provided in Appendix Table~\ref{tab:eval_diag_loops_iface_counts}.

\emph{(i) Centralised scalability is driven by dissemination tail, not solver load.}
For the centralised backend, total traffic grows with team size primarily because the global map broadcast must serve more robots and the map itself grows as more trajectories contribute constraints. Under WiFi, traffic increases from 55.0\,MiB (r3) to 69.0\,MiB (r4) and 91.0\,MiB (r5). Under ProRadio, the same monotonic growth is observed (30.3\,MiB to 74.4\,MiB to 86.8\,MiB), and \(t_{\mathrm{global}}\) increases accordingly. Importantly, solver stability (as measured by \(T_{\mathrm{conv}}^{(\mathrm{team})}\)) remains low across all team sizes, indicating that the centralised optimiser is not the bottleneck.

\emph{(ii) Decentralised scalability is regime-dependent, not uniform.}
Under WiFi, decentralised traffic grows from 28.7\,MiB (r3) to 41.4\,MiB (r4) and 66.9\,MiB (r5), and accuracy degrades at r5 (ATE RMSE 6.66\,m), suggesting that interface coordination becomes a limiting factor for maintaining global consistency at this team size under the given dataset regime. Under ProRadio, decentralised performance is comparatively stable across team sizes: ATE RMSE is 3.82\,m (r3), 2.72\,m (r4), and 2.90\,m (r5), while traffic grows from 32.2\,MiB to 36.1\,MiB and 47.4\,MiB.

\emph{(iii) Constraint structure mediates scalability more than robot count.}
Although WiFi exhibits more inter-robot loop closures than ProRadio (Appendix Table~\ref{tab:eval_diag_loops_iface_counts}), WiFi r5 shows worse decentralised accuracy than ProRadio r5. This suggests that the \emph{distribution and timing} of constraints, rather than merely their count, shape how effectively distributed coordination can maintain global consistency. Figure~\ref{fig:eval_pareto_ate_vs_traffic} complements this trend view by showing the per-repeat ATE--traffic dispersion and the resulting trade-off frontier.

\begin{table}[H]
	\centering
	\caption{Condensed baseline scalability trend across team sizes (r3--r5). Link: P=ProRadio, W=WiFi. Backend: C=centralised, D=decentralised. ``Time'' denotes \(t_{\mathrm{global}}\) for centralised runs (dissemination proxy) and Corr.\ p95 for decentralised runs (correction-propagation proxy). \(T_{\mathrm{conv}}^{(\mathrm{team})}\) is the shared stabilisation proxy (mean $\pm$ 95\% CI; ``$\geq$'' indicates right-censoring when stabilisation is not observed).}
	\label{tab:eval_baseline_scalability_condensed}
	\small
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{lllc r r r r}
		\toprule
		Link & Team & B & Status & ATE RMSE (m) & Time (s) & \(T_{\mathrm{conv}}^{(\mathrm{team})}\) (s) & Traffic (MiB) \\
		\midrule
		P & r3 & C & ok & 2.164 $\pm$ 0.000 & 11.5 & 0.01 $\pm$ 0.00 & 30.3 \\
		P & r4 & C & ok & 2.526 $\pm$ 0.000 & 21.6 & 0.12 $\pm$ 0.01 & 74.4 \\
		P & r5 & C & ok & 1.985 $\pm$ 0.013 & 25.0 & 0.17 $\pm$ 0.00 & 86.8 \\
		P & r3 & D & ok & 3.821 $\pm$ 0.072 & 14.4 & $\geq$ 21.33 $\pm$ 5.94 & 32.2 \\
		P & r4 & D & ok & 2.721 $\pm$ 0.030 & 8.3  & 12.58 $\pm$ 1.20 & 36.1 \\
		P & r5 & D & ok & 2.898 $\pm$ 0.028 & 6.0  & $\geq$ 20.34 $\pm$ 10.35 & 47.4 \\
		\midrule
		W & r3 & C & ok & 2.270 $\pm$ 0.002 & 16.4 & 0.12 $\pm$ 0.00 & 55.0 \\
		W & r4 & C & ok & 2.151 $\pm$ 0.000 & 20.6 & 0.13 $\pm$ 0.00 & 69.0 \\
		W & r5 & C & ok & 1.885 $\pm$ 0.031 & 26.3 & 0.15 $\pm$ 0.01 & 91.0 \\
		W & r3 & D & ok & 3.581 $\pm$ 0.035 & 4.8  & 15.19 $\pm$ 3.53 & 28.7 \\
		W & r4 & D & ok & 3.274 $\pm$ 0.030 & 4.9  & 11.53 $\pm$ 0.45 & 41.4 \\
		W & r5 & D & ok & 6.656 $\pm$ 0.635 & 6.2  & 23.85 $\pm$ 1.55 & 66.9 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.98\linewidth]{src/img/main/eval_scalability.pdf}
	\caption{Scalability trend in the baseline sweep (Table~\ref{tab:eval_baseline_scalability_condensed}): ATE RMSE and traffic vs.\ team size (r3--r5) for WiFi and ProRadio. Lines show mean over repeats ($R=5$); error bars denote 95\% CI across repeats.}
	\label{fig:eval_scalability}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.90\linewidth]{src/img/main/eval_pareto_ate_vs_traffic.pdf}
	\caption{Pareto view of baseline trade-offs across team sizes (r3--r5): ATE RMSE vs.\ total traffic for each successful repeat. This visualisation complements Figure~\ref{fig:eval_scalability} by showing per-run dispersion and the accuracy--bandwidth frontier for each architecture.}
	\label{fig:eval_pareto_ate_vs_traffic}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.98\linewidth]{src/img/main/eval_ddf_ate_vs_stop_time.pdf}
	\caption{Decentralised diagnostic: ATE mean vs.\ DDF termination time. The x-axis reports \(T_{\mathrm{stop}}^{(\mathrm{team})}\) (time from \texttt{input\_end} until all agents emit \texttt{ddf\_stop}, team statistic = max over robots). Error bars denote 95\% CI over repeats ($R=5$).}
	\label{fig:eval_ddf_ate_vs_stop_time}
\end{figure}

\section{Sensitivity to ROS\,2 QoS Policy}
\label{sec:eval_qos}

The QoS study isolates factor \(\mathcal{Q}\) by changing only the ROS\,2 communication profile while keeping the dataset regime and all other parameters fixed. Two alternate profiles are evaluated: BT-TL-10 and BT-TL-50, which use best-effort reliability and transient-local durability with history depth 10 and 50, respectively. This tests whether relaxing delivery guarantees (best-effort) and modifying buffering (depth) alter dissemination/correction timing, bandwidth, or estimation quality.

\emph{(i) QoS changes do not materially affect centralised operation in the evaluated settings.}
Table~\ref{tab:eval_qos_r3} shows the r3 results. For the centralised backend, QoS changes are negligible: ATE and traffic remain unchanged within rounding and \(t_{\mathrm{global}}\) remains close to the baseline values.

\emph{(ii) QoS primarily influences decentralised bandwidth overhead through buffering depth.}
For the decentralised backend, QoS mainly influences bandwidth through history depth. On WiFi r3, BT-TL-50 increases total traffic by about 35.5\% relative to BT-TL-10 (33.6\,MiB vs.\ 24.8\,MiB). Despite this shift in communication volume, the ATE change remains small, suggesting that the evaluated QoS settings affect buffering overhead more than estimation quality in the nominal regime for the successful WiFi r3 runs.

\emph{(iii) One decentralised configuration is unsupported, bounding QoS generality.}
One decentralised QoS configuration (ProRadio r3 under BT-TL-10) does not complete successfully due to an agent crash (Appendix Table~\ref{tab:eval_excluded_runs}). This point is treated as an unsupported configuration in the current implementation and bounds conclusions about QoS sensitivity under that regime/profile pair. Figure~\ref{fig:eval_qos_slope_chart} provides a compact visual summary of these QoS shifts in accuracy and bandwidth relative to baseline.

\begin{table}[H]
	\centering
	\caption{QoS sensitivity at r3. Two alternate profiles are evaluated: BT-TL-10 and BT-TL-50, which use best-effort reliability and transient-local durability with history depth 10 and 50, respectively. Baseline (RV-20) results are reported in Table~\ref{tab:eval_baseline_r3}. ``Time'' denotes \(t_{\mathrm{global}}\) for centralised runs and Corr.\ p95 for decentralised runs. \(T_{\mathrm{conv}}^{(\mathrm{team})}\) is the shared stabilisation proxy (mean $\pm$ 95\% CI; ``$\geq$'' indicates right-censoring). Rows marked fail indicate at least one repeat crashed.
		Link(L) = WiFi(W) \& ProRadio(P); QoS profile = BT-TL-10/50}
	\label{tab:eval_qos_r3}
	\small
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{llllcccc}
		\toprule
		L & B/E & QoS & Status & ATE mean (m) & Traffic (MiB) & Time (s) & $T_{\mathrm{conv}}^{(\mathrm{team})}$ (s) \\
		\midrule
		W & C & 10 & ok & 2.272 $\pm$ 0.000 & 55.0 & 16.4 & 0.12 $\pm$ 0.00 \\
		W & C & 50 & ok & 2.272 $\pm$ 0.000 & 55.0 & 16.4 & 0.12 $\pm$ 0.00 \\
		W & D & 10 & ok & 3.598 $\pm$ 0.032 & 24.8 & 7.0 & 17.12 $\pm$ 1.45 \\
		W & D & 50 & ok & 3.578 $\pm$ 0.030 & 33.6 & 4.4 & 13.49 $\pm$ 1.08 \\
		P & C & 10 & ok & 2.164 $\pm$ 0.000 & 30.3 & 11.7 & 0.01 $\pm$ 0.00 \\
		P & C & 50 & ok & 2.164 $\pm$ 0.000 & 30.3 & 11.7 & 0.01 $\pm$ 0.00 \\
		P & D & 10 & fail & -- & -- & -- & -- \\
		P & D & 50 & ok & 3.859 $\pm$ 0.038 & 37.2 & 12.7 & $\geq$ 24.54 $\pm$ 1.09 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.98\linewidth]{src/img/main/eval_qos_sensitivity_slopes.pdf}
	\caption{QoS sensitivity (r3): summary of how ATE and traffic shift from the baseline profile (RV-20) under BT-TL-10 and BT-TL-50. Lines show mean over repeats ($R=5$); error bars denote 95\% CI across repeats. Failed configurations are marked.}
	\label{fig:eval_qos_slope_chart}
\end{figure}

\section{Sensitivity to Synthetic Network Impairments}
\label{sec:eval_impair}

The impairment study varies \(\mathcal{I}\) by injecting controlled degradations (Section~\ref{sec:network_impairment_qos}). In the reported sweep, impairments are applied to factor publication (uplink/factor-stream degradation). Two impairment families are considered: bandwidth caps (0.25--3.0\,Mbps) and periodic blackouts (\texttt{blackout\_2x}). The evaluation asks not only “does error increase?”, but also \emph{which latency proxy moves, and why}.

\subsection{Centralised Backend Under Impairments}
\label{subsec:eval_impair_centralised}

For the centralised backend, bandwidth caps on the factor stream have an intuitive effect: they do not change the total information volume (bytes transmitted remains essentially constant), but they delay when batches arrive at the server and therefore extend the map-broadcast tail.

\emph{(i) Severe bandwidth caps inflate dissemination tail without changing total traffic.}
With the most restrictive cap (0.25\,Mbps), \(t_{\mathrm{global}}\) increases by factors of 9.6$\times$ (WiFi r3) and 10.3$\times$ (ProRadio r3), and by 8.0$\times$ (WiFi r5) and 8.1$\times$ (ProRadio r5). In contrast, caps of 1--3\,Mbps are nearly indistinguishable from baseline in both timing and accuracy for the evaluated datasets.

\emph{(ii) Blackouts reduce delivery and can increase ATE, but runs remain stable.}
Blackout impairments primarily affect delivery rather than throughput. In the centralised pipeline, publisher-side blackouts reduce factor-stream delivery rates and can change estimation error when critical constraints are missed or delayed, but runs can still complete because the optimiser continues to operate on the received subset of factors and periodically broadcasts the map. Table~\ref{tab:eval_impair_centralised} summarises these effects.

\begin{table}[H]
	\centering
	\caption{Impact of representative network impairments on the centralised backend. Bandwidth caps of 1--3\,Mbps were near-identical to baseline and are omitted here; only the most restrictive cap (0.25\,Mbps) is shown. Wr3/5 = WiFi r3/5; Pr3/5 = ProRadio r3/5; bwcap = bwcap 0.25 Mbps; FDM = Factor Delivery Time}
	\label{tab:eval_impair_centralised}
	\small
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{llcccc}
		\toprule
		Scenario & Impairment & $t_{\mathrm{global}}$ (s) & ATE mean (m) & FDM & Traffic (MiB) \\
		\midrule
		Wr3 & none & 16.4 & 2.270 $\pm$ 0.002 & 1.000 & 55.0 \\
		Wr3 & bwcap & 158.1 & 2.272 $\pm$ 0.000 & 1.000 & 55.0 \\
		Wr3 & blackout & 10.2 & 3.192 $\pm$ 0.243 & 0.499 & 24.3 \\
		Wr5 & none & 26.3 & 1.885 $\pm$ 0.031 & 1.000 & 91.0 \\
		Wr5 & bwcap & 210.6 & 1.902 $\pm$ 0.000 & 1.000 & 90.9 \\
		Wr5 & blackout & 7.9 & 1.765 $\pm$ 0.058 & 0.263 & 22.8 \\
		Pr3 & none & 11.5 & 2.164 $\pm$ 0.000 & 1.000 & 30.3 \\
		Pr3 & bwcap & 118.4 & 2.164 $\pm$ 0.000 & 1.000 & 30.3 \\
		Pr3 & blackout & 11.7 & 1.722 $\pm$ 0.093 & 0.742 & 26.9 \\
		Pr5 & none & 25.0 & 1.985 $\pm$ 0.013 & 1.000 & 86.8 \\
		Pr5 & bwcap & 202.0 & 1.990 $\pm$ 0.000 & 1.000 & 86.8 \\
		Pr5 & blackout & 21.2 & 1.652 $\pm$ 0.028 & 0.400 & 74.1 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Decentralised Backend Under Impairments}
\label{subsec:eval_impair_decentralised}

For the decentralised backend, impairment response is more regime-dependent. Because the current sweep impairs only factor publication, the peer-to-peer interface exchange is not directly impaired; instead, bandwidth caps and blackouts change the availability and timing of new constraints at each agent.

\emph{(i) Decentralised timing response to bandwidth caps is mixed and depends on regime.}
Under WiFi, the evaluated blackout condition leads to agent failures (Appendix Table~\ref{tab:eval_excluded_runs}), while bandwidth caps produce only modest changes in ATE and traffic for the successful runs. Under ProRadio, the strict 0.25\,Mbps cap increases interface correction stabilisation latency (p95) and increases ATE at r3, indicating sensitivity to delayed constraint ingestion and slower correction settling under reduced factor throughput.

\emph{(ii) Blackouts trigger stability failures in multiple configurations.}
Blackouts are survivable at ProRadio r3 but still trigger agent failures at r4 and r5. WiFi blackouts fail at r3 and r5. These failures indicate that robustness conclusions for decentralised operation under intermittent connectivity are conditional on the stability/supportability of the current implementation. Table~\ref{tab:eval_impair_decentralised} summarises the decentralised impairment results. Figure~\ref{fig:eval_impairment_degradation} visualises the normalised slowdown (relative to baseline) for both architectures across impairment conditions.

\begin{table}[H]
	\centering
	\caption{Impact of representative network impairments on the decentralised backend (DDF-style). ``Iface p95'' is interface correction stabilisation latency (p95). Rows marked fail indicate runs where at least one agent crashed.
		Wr3/5 = WiFi r3/5; Pr3/5 = ProRadio r3/5; bwcap = bwcap 0.25 Mbps; FDM = Factor Delivery Time}
	
	\label{tab:eval_impair_decentralised}
	\small
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{lllcccc}
		\toprule
		Scenario & Impairment & Status & ATE mean (m) & Iface p95 (s) & FDM & Traffic (MiB) \\
		\midrule
		Wr3 & none & ok & 3.581 $\pm$ 0.035 & 4.8 & 0.995 & 28.7 \\
		Wr3 & bwcap & ok & 3.465 $\pm$ 0.024 & 9.5 & 0.989 & 28.0 \\
		Wr3 & blackout & fail & -- & -- & -- & -- \\
		Wr5 & none & ok & 6.656 $\pm$ 0.635 & 6.2 & 0.998 & 66.9 \\
		Wr5 & bwcap & ok & 7.603 $\pm$ 0.000 & 1.6 & 0.944 & 64.2 \\
		Wr5 & blackout & fail & -- & -- & -- & -- \\
		Pr3 & none & ok & 3.821 $\pm$ 0.072 & 14.4 & 1.000 & 32.2 \\
		Pr3 & bwcap & ok & 4.297 $\pm$ 0.044 & 22.7 & 0.990 & 28.9 \\
		Pr3 & blackout & ok & 2.768 $\pm$ 0.031 & 10.3 & 0.722 & 29.7 \\
		Pr5 & none & ok & 2.898 $\pm$ 0.028 & 6.0 & 1.000 & 47.4 \\
		Pr5 & bwcap & ok & 4.876 $\pm$ 0.000 & 1.4 & 0.980 & 41.8 \\
		Pr5 & blackout & fail & -- & -- & -- & -- \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.98\linewidth]{src/img/main/eval_impairment_degradation.pdf}
	\caption{Normalised impairment sensitivity (slowdown vs.\ baseline) under uplink-only (publisher-impaired) degradations (Tables~\ref{tab:eval_impair_centralised} and~\ref{tab:eval_impair_decentralised}): centralised \(t_{\mathrm{global}}/t_{\mathrm{base}}\) and decentralised Iface p95$/t_{\mathrm{base}}$. Points show mean over repeats ($R=5$); error bars denote 95\% CI across repeats. Failed runs are marked.}
	\label{fig:eval_impairment_degradation}
\end{figure}

\section{Robustness and Unsupported Configurations}
\label{sec:eval_robustness}

Appendix Table~\ref{tab:eval_excluded_runs} lists configurations that did not complete successfully and were excluded from quantitative comparisons. These outcomes bound the generality of conclusions about QoS and impairment sensitivity for the decentralised backend.

The key robustness takeaway is not merely that ``blackouts are hard'' (which is expected), but that the observed decentralised failures are triggered by a \emph{specific implementation fragility} under intermittent or delayed delivery illustrates an engineering limitation. Figure~\ref{fig:eval_robustness_matrix} provides a compact visual summary of which configurations are supported vs.\ unsupported across the evaluated design space.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.90\linewidth]{src/img/main/eval_robustness_matrix.pdf}
	\caption{Robustness matrix summarising which configurations are supported (all repeats ok) vs.\ unsupported (at least one repeat fails) across the three studies: baseline, QoS, and impairments. Green \texttt{ok} indicates all repeats completed successfully; red \texttt{fail} indicates at least one repeat failed; grey \texttt{n/a} indicates a configuration not evaluated in this study (QoS is evaluated at r3 only; impairments at r3 and r5 only).}
	\label{fig:eval_robustness_matrix}
\end{figure}

\subsection{Failure Mode Analysis (decentralised crashes)}
\label{subsec:eval_failure_modes}

Across the failed decentralised runs in Appendix Table~\ref{tab:eval_excluded_runs}, the crash signature is consistent: an out-of-range index in GTSAM iSAM2 factor removal. The exported traces indicate that the exception is raised during \texttt{isam.update(..., removeFactorIndices=...)} and is triggered from the decentralised agent update loop. This points to an implementation-level bookkeeping bug (factor-removal indices becoming inconsistent with the internal factor container), rather than an estimator-level divergence.

Resource traces provide supporting context: in representative impairment failures, one or more agents exhibit monotonic RSS growth and high CPU bursts during prolonged intermittency, consistent with queue/backlog accumulation. These conditions plausibly increase the likelihood of stale or invalid factor-removal indices (e.g., through out-of-order updates or mismatched bookkeeping across rounds). In this thesis, these points are treated conservatively as unsupported configurations; claims about decentralised robustness under intermittent connectivity are therefore conditional on resolving this stability issue. Supporting time-series traces are provided in Appendix Figure~\ref{fig:eval_resources_timeseries}.

\section{Synthesis and Limitations}
\label{sec:eval_synthesis}

The evaluation supports four conclusions aligned with the research questions. Each answer identifies the dominant factor that shapes the observed behaviour, rather than merely reporting metric values.

\paragraph{RQ1 (latency and convergence): what dominates ``time to a usable update''?}
Compute time per update is not the dominant contributor in these runs. Centralised updates stabilise rapidly once disseminated (small \(T_{\mathrm{conv}}^{(\mathrm{team})}\)), so perceived latency is largely driven by dissemination-tail behaviour (\(t_{\mathrm{global}}\)). Decentralised updates can be compute-cheap but exhibit longer stabilisation tails; perceived latency is therefore dominated by correction propagation and settling.

\paragraph{RQ2 (team size): what actually scales with more robots?}
For the centralised backend, the key scaling pressure is dissemination payload and the broadcast tail, not solver stabilisation. For the decentralised backend, scaling is regime-dependent: in ProRadio, r4--r5 remain stable with moderate traffic growth, whereas WiFi r5 shows a marked accuracy degradation despite increased interface activity. This indicates that scalability is mediated by constraint structure and coordination dynamics, not solely by team size.

\paragraph{RQ3 (bandwidth/compute under constraints): what changes under uplink degradation?}
In publisher-impaired mode, centralised runs retain similar traffic volume under bandwidth caps but pay in latency (large increases in \(t_{\mathrm{global}}\)). Decentralised runs show mixed timing responses under caps because missing/delayed constraints can reduce correction work while harming accuracy; therefore, constraint availability must be considered alongside timing proxies. QoS tuning primarily affects decentralised bandwidth overhead rather than accuracy in nominal conditions.

\paragraph{RQ4 (robustness): where do failures come from?}
Under intermittent factor availability, centralised runs complete (with reduced delivery and sometimes increased ATE). Several decentralised configurations fail due to a repeatable factor-removal bookkeeping error. This bounds robustness claims for the decentralised backend in the current implementation and highlights a practical requirement for deployment: robustness must include both estimator-level resilience and software-level stability under backlog and out-of-order conditions.

\paragraph{Limitations}
Two limitations are most relevant. First, the impairment results are scoped to publisher-impaired mode; a complementary sweep that directly impairs decentralised peer exchange and/or centralised downlink would be required to attribute sensitivity to those communication edges. Second, unsupported decentralised configurations limit generality under severe intermittency; the failure analysis identifies a concrete stability issue that should be resolved to fairly characterise decentralised robustness beyond ``does it crash?''.

\paragraph{Note on RPE}
Relative pose error (RPE) is defined in Section~\ref{sec:metrics} as a drift-oriented metric; this chapter focuses on ATE to keep tables compact. RPE values are available in the KPI exports and can be reported in an appendix if required.
